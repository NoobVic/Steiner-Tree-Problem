{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the dataframe path and file type\n",
    "df_path = \"../df/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and save dataframe\n",
    "import pandas as pd \n",
    "def read_df(file_name):\n",
    "    df = pd.read_csv(df_path+file_name)\n",
    "    return df\n",
    "\n",
    "def save_df(df, file_name):\n",
    "    df.to_csv(df_path+file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file list\n",
    "import os\n",
    "files = os.listdir(df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each feature computation, the steps are:\n",
    "#### 1: Read each dataframe from .csv files\n",
    "#### 2: Calculate the feature and save it as a new column\n",
    "#### 3: Write the new dataframe back to the original .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature(func):\n",
    "    for file in files:\n",
    "        # Read the file into dataframe\n",
    "        df = read_df(file)\n",
    "        # Update the dataframe by a new feature column\n",
    "        func(df)\n",
    "        # Save back to the file\n",
    "        save_df(df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_col(col_name):\n",
    "     for file in files:\n",
    "        # Read the file into dataframe\n",
    "        df = read_df(file)\n",
    "        # Update the dataframe by remove a current column\n",
    "        df = df.drop(columns=col_name)\n",
    "        # Save back to the file\n",
    "        save_df(df, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Type 1: Weight related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 1.1: Normalized weight by dividing the max weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nor_by_max(df):\n",
    "    tmp = df['weight'].max()\n",
    "    col = df['weight'] / tmp\n",
    "    df.insert(len(df.columns)-1,'weight_nor_max',col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_feature(nor_by_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 1.2: Normalized weight by the absolute weight difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nor_by_abs(df):\n",
    "    tmp_min = df['weight'].min()\n",
    "    tmp_max = df['weight'].max()\n",
    "    col = ( df['weight'] - tmp_min ) / (tmp_max - tmp_min )\n",
    "    df.insert(len(df.columns)-1,'weight_nor_abs',col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_feature(nor_by_abs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 1.3: Edge weight normalized chi-square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_variance(df):\n",
    "    # Use the normailzed weight for calculation\n",
    "    avg = df['weight_nor_abs'].mean()\n",
    "    col = (df['weight_nor_abs'] - avg) * (df['weight_nor_abs'] - avg).abs()\n",
    "    # Normalize to [0,1]\n",
    "    col -= col.min()\n",
    "    df.insert(len(df.columns)-1,'weight_abs_var',col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_feature(abs_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 1.4: Local rank for edge (i,j) on vertex i and vertex j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_rank(df):\n",
    "    # Loop every vertex\n",
    "    df.insert(len(df.columns)-1,'loc_rank_i',np.zeros(len(df),dtype=np.int8))\n",
    "    df.insert(len(df.columns)-1,'loc_rank_j',np.zeros(len(df),dtype=np.int8))\n",
    "    for i in range(1, max(df['v1'].max(),df['v2'].max()) + 1):\n",
    "        loc_edges = df.loc[(df['v1'] == i) | (df['v2'] == i)]\n",
    "        loc_edges = loc_edges.sort_values(by=['weight'])\n",
    "        loc_edges = loc_edges.reset_index(drop=True)\n",
    "        for index, row in loc_edges.iterrows():\n",
    "            # When the current vertex is the i vertex for this edge\n",
    "            if( row['v1'] == i):\n",
    "                df.loc[(df['v1'] == row['v1']) & (df['v2'] == row['v2']), ['loc_rank_i']] = (index + 1)/(loc_edges.index.max() + 1)\n",
    "            # When the current vertex is the j vertex for this edge\n",
    "            else:\n",
    "                df.loc[(df['v1'] == row['v1']) & (df['v2'] == row['v2']), ['loc_rank_j']] = (index + 1)/(loc_edges.index.max() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# calculate_feature(loc_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Type 2: Centrality related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this part, most features are calculated by NetworkX build-in functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 2.1 Degree Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_cen(df):\n",
    "    # Create Graph object\n",
    "    G = nx.Graph()\n",
    "    for index, row in df.iterrows():\n",
    "        i = row['v1']\n",
    "        j = row['v2']\n",
    "        G.add_edge(i,j)\n",
    "    cen = nx.degree_centrality(G)\n",
    "    df.insert(len(df.columns)-1,'degree_cen_i',np.zeros(len(df)))\n",
    "    df.insert(len(df.columns)-1,'degree_cen_j',np.zeros(len(df)))\n",
    "    for key in cen:\n",
    "        df.loc[df['v1'] == key, 'degree_cen_i'] = cen[key]\n",
    "        df.loc[df['v2'] == key, 'degree_cen_j'] = cen[key]\n",
    "    \n",
    "    df.insert(len(df.columns)-1,'degree_cen_max',np.zeros(len(df)))\n",
    "    df.insert(len(df.columns)-1,'degree_cen_min',np.zeros(len(df)))\n",
    "    for index, row in df.iterrows():\n",
    "        df.loc[index, 'degree_cen_max'] = max(row['degree_cen_i'], row['degree_cen_j'])\n",
    "        df.loc[index, 'degree_cen_min'] = min(row['degree_cen_i'], row['degree_cen_j'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_feature(degree_cen)\n",
    "# drop_col('degree_cen_i')\n",
    "# drop_col('degree_cen_j')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 2.2 Betweenness Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def between_cen(df):\n",
    "    # Create Graph object\n",
    "    G = nx.Graph()\n",
    "    for index, row in df.iterrows():\n",
    "        i = row['v1']\n",
    "        j = row['v2']\n",
    "        w = row['weight']\n",
    "        G.add_edge(i,j,weight=w)\n",
    "    cen = nx.betweenness_centrality(G,k=10,normalized=True,weight='weight')\n",
    "    df.insert(len(df.columns)-1,'between_cen_i',np.zeros(len(df)))\n",
    "    df.insert(len(df.columns)-1,'between_cen_j',np.zeros(len(df)))\n",
    "    for key in cen:\n",
    "        df.loc[df['v1'] == key, 'between_cen_i'] = cen[key]\n",
    "        df.loc[df['v2'] == key, 'between_cen_j'] = cen[key]\n",
    "    \n",
    "    df.insert(len(df.columns)-1,'between_cen_max',np.zeros(len(df)))\n",
    "    df.insert(len(df.columns)-1,'between_cen_min',np.zeros(len(df)))\n",
    "    for index, row in df.iterrows():\n",
    "        df.loc[index, 'between_cen_max'] = max(row['between_cen_i'], row['between_cen_j'])\n",
    "        df.loc[index, 'between_cen_min'] = min(row['between_cen_i'], row['between_cen_j'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_feature(between_cen)\n",
    "# drop_col('between_cen_i')\n",
    "# drop_col('between_cen_j')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 2.3 Vote Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_rank(df):\n",
    "    # Create Graph object\n",
    "    G = nx.Graph()\n",
    "    for index, row in df.iterrows():\n",
    "        i = row['v1']\n",
    "        j = row['v2']\n",
    "        w = row['weight']\n",
    "        G.add_edge(i,j,weight=w)\n",
    "    rank = nx.voterank(G)\n",
    "    df.insert(len(df.columns)-1,'vote_rank_i',np.zeros(len(df)))\n",
    "    df.insert(len(df.columns)-1,'vote_rank_j',np.zeros(len(df)))\n",
    "    for v in rank:\n",
    "        df.loc[df['v1'] == v, 'vote_rank_i'] = 1\n",
    "        df.loc[df['v2'] == v, 'vote_rank_j'] = 1\n",
    "        \n",
    "    df.insert(len(df.columns)-1,'vote_rank_min',np.zeros(len(df)))\n",
    "    df.insert(len(df.columns)-1,'vote_rank_max',np.zeros(len(df)))\n",
    "    for index, row in df.iterrows():\n",
    "        df.loc[index, 'vote_rank_max'] = max(row['vote_rank_i'], row['vote_rank_j'])\n",
    "        df.loc[index, 'vote_rank_min'] = min(row['vote_rank_i'], row['vote_rank_j'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_feature(vote_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found this feature useless from my evaluation\n",
    "# drop_col(\"vote_rank_i\")\n",
    "# drop_col(\"vote_rank_j\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 2.4 Eigenvector Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigen_cen(df):\n",
    "    # Create Graph object\n",
    "    G = nx.Graph()\n",
    "    for index, row in df.iterrows():\n",
    "        i = row['v1']\n",
    "        j = row['v2']\n",
    "        w = row['weight']\n",
    "        G.add_edge(i,j,weight=w)\n",
    "    cen = nx.eigenvector_centrality(G,max_iter=99999,weight='weight')\n",
    "    df.insert(len(df.columns)-1,'eigen_cen_i',np.zeros(len(df)))\n",
    "    df.insert(len(df.columns)-1,'eigen_cen_j',np.zeros(len(df)))\n",
    "    for key in cen:\n",
    "        df.loc[df['v1'] == key, 'eigen_cen_i'] = cen[key]\n",
    "        df.loc[df['v2'] == key, 'eigen_cen_j'] = cen[key]\n",
    "        \n",
    "    df.insert(len(df.columns)-1,'eigen_cen_max',np.zeros(len(df)))\n",
    "    df.insert(len(df.columns)-1,'eigen_cen_min',np.zeros(len(df)))\n",
    "    for index, row in df.iterrows():\n",
    "        df.loc[index, 'eigen_cen_max'] = max(row['eigen_cen_i'], row['eigen_cen_j'])\n",
    "        df.loc[index, 'eigen_cen_min'] = min(row['eigen_cen_i'], row['eigen_cen_j'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_feature(eigen_cen)\n",
    "# drop_col('eigen_cen_i')\n",
    "# drop_col('eigen_cen_j')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Type 3: Linear Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and save dataframe\n",
    "import pandas as pd \n",
    "def read_df(file_name):\n",
    "    df = pd.read_csv(\"../df/\"+file_name+\".csv\")\n",
    "    return df\n",
    "\n",
    "def save_df(df, file_name):\n",
    "    df.to_csv(\"../df/\"+file_name+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def read_graph(name):\n",
    "    with open(name) as f:\n",
    "        lines = f.readlines()\n",
    "        arcs = []\n",
    "        for line in lines:\n",
    "            if line == '\\n': \n",
    "                continue\n",
    "            parts = line.split()\n",
    "            det = parts[0]\n",
    "            if det == 'Name':\n",
    "                name = parts[1]\n",
    "            elif det == 'Nodes':\n",
    "                n_vertices = int(parts[1])\n",
    "            elif det == 'Edges':\n",
    "                n_edges = int(parts[1])\n",
    "            elif det == 'E':\n",
    "                i = int(parts[1])\n",
    "                j = int(parts[2])\n",
    "                c = int(parts[3])\n",
    "                arcij = ((i,j),c)\n",
    "                arcji = ((j,i),c)\n",
    "                arcs.append(arcij)\n",
    "                arcs.append(arcji)\n",
    "            elif det == 'Terminals':\n",
    "                n_terminals = int(parts[1])\n",
    "        vertices = np.arange(1, int(n_vertices)+1)\n",
    "        vertices = vertices.tolist()\n",
    "        terminals = np.arange(1, int(n_terminals)+1)\n",
    "        terminals = terminals.tolist()\n",
    "        assert(int(n_edges) == len(arcs)/2)\n",
    "    f.close()\n",
    "    ### The format of graphs is dG=(V,dE,Z) \n",
    "    return [vertices, arcs, terminals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "graphs = {}\n",
    "path = \"../ds/\"\n",
    "size = \"I080/\"\n",
    "files = os.listdir(path+size)\n",
    "for file in files:\n",
    "    file_name = file[:-4]\n",
    "    graph = read_graph(path+size+file)\n",
    "    graphs[file_name] = graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "def get_lp(graph):\n",
    "    # read dG=(V,dE,Z) \n",
    "    vertices, arcs, terminals = graph\n",
    "    \n",
    "    # obtain the size of each set\n",
    "    n = len(vertices) # n = number of vertices\n",
    "    m = len(arcs)/2 # m = number of edges\n",
    "    p = len(terminals) # p = number of ternimals\n",
    "    \n",
    "    # choose 1 out of ternimals as the source vertex\n",
    "    v0 = terminals[0] \n",
    "\n",
    "    # delete all the arcs that enter the source vertex\n",
    "    arcs = [arc for arc in arcs if not arc[0][1] == v0]\n",
    "\n",
    "    # create the tuple dictionary of arcs\n",
    "    arcs_dict = gp.tupledict(arcs)\n",
    "    \n",
    "    # create the model\n",
    "    m = gp.Model(\"Steiner\")\n",
    "    \n",
    "    # create the decision variables\n",
    "    # number of variables: 2m + m - 1\n",
    "    x = m.addVars(arcs_dict.keys(),lb=0,ub=1,vtype=GRB.CONTINUOUS, name='x')\n",
    "    u = m.addVars(vertices,lb=-1,ub=n, vtype=GRB.INTEGER, name='u' )\n",
    "    \n",
    "    # set up the objective function\n",
    "    # equation (1)\n",
    "    m.setObjective(gp.quicksum(arcs_dict[i, j] * x[i, j] for (i, j) in arcs_dict.keys()), GRB.MINIMIZE)\n",
    "    \n",
    "    # create the constraints\n",
    "    vertices = vertices[:v0-1] + vertices[v0:]\n",
    "    for j in vertices: # number: 3n - 3\n",
    "        # equation (2)\n",
    "        m.addConstr(x.sum('*', j) <= 1)\n",
    "        # equation (3)\n",
    "        m.addConstr(n * x.sum('*', j) >= u[j] + 1)\n",
    "        # equation (4)\n",
    "        m.addConstr((n + 1) * x.sum('*', j) <= n * (u[j] + 1))\n",
    "        \n",
    "    for ij in arcs_dict.keys(): # number: 4m\n",
    "        i = ij[0]\n",
    "        j = ij[1]\n",
    "        # equation (6)\n",
    "        m.addConstr(1 - n * (1 - x[i, j] ) <= u[j] - u[i])\n",
    "        m.addConstr(1 + n * (1 - x[i, j] ) >= u[j] - u[i])\n",
    "    \n",
    "    ternimals = terminals[1:]\n",
    "    for j in terminals: # number: p - 1\n",
    "        # equation (7)\n",
    "        m.addConstr( u[j] >= 0 )\n",
    "    \n",
    "    # equation (7)    \n",
    "    m.addConstr( u[v0] == 0 )\n",
    "    \n",
    "    # update the model\n",
    "    m.update()\n",
    "    \n",
    "    # optimize the model\n",
    "    m.optimize()\n",
    "    \n",
    "    # get the optimal value\n",
    "    edges = []\n",
    "    for v in m.getVars():\n",
    "        # save the edges\n",
    "        if v.varName.startswith('x'):\n",
    "            i,j = v.varName[2:-1].split(',')\n",
    "            edge = ((int(i),int(j)), v.x)\n",
    "            edges.append(edge)\n",
    "    \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in files:\n",
    "#     # Read the graph of dataset first\n",
    "#     file_name = file[:-4]\n",
    "#     graph = graphs[file_name]\n",
    "#     edges = get_lp(graph)\n",
    "#     df = pd.read_csv(df_path+file_name+\".csv\")\n",
    "#     df.insert(len(df.columns)-1,'lp_value_i_j',np.zeros(len(df)))\n",
    "#     df.insert(len(df.columns)-1,'lp_value_j_i',np.zeros(len(df)))\n",
    "#     for (i, j), v in edges:\n",
    "#         df.loc[(df['v1'] == i) & (df['v2'] == j), 'lp_value_i_j'] = v\n",
    "#         df.loc[(df['v2'] == i) & (df['v1'] == j), 'lp_value_j_i'] = v\n",
    "#     df.insert(len(df.columns)-1,'lp_value',np.zeros(len(df)))\n",
    "#     for index, row in df.iterrows():\n",
    "#         df.loc[index, 'lp_value'] = max(row['lp_value_i_j'], row['lp_value_j_i'])\n",
    "#     df = df.drop(columns='lp_value_i_j')\n",
    "#     df = df.drop(columns='lp_value_j_i')\n",
    "#     save_df(df, file_name+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_col(\"vote_rank_max\")\n",
    "# drop_col(\"vote_rank_min\")\n",
    "# drop_col(\"lp_value\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
