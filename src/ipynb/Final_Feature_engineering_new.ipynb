{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a062a3df",
   "metadata": {},
   "source": [
    "## Variables Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "log_path = '../log/'\n",
    "df_path = '../df/'\n",
    "ds_path = '../ds/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af02c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suffix\n",
    "log_suffix = '_log.txt'\n",
    "ds_suffix = '.stp'\n",
    "df_suffix = '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8406e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read/Write Dataframe \n",
    "import pandas as pd \n",
    "def read_df(target):\n",
    "    df = pd.read_csv(target)\n",
    "    return df\n",
    "\n",
    "def write_df(df, target):\n",
    "    df.to_csv(target, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get File lists\n",
    "import os\n",
    "tmp = os.listdir(log_path)\n",
    "\n",
    "log_dict = {}\n",
    "for folder in tmp:\n",
    "    log_dict.update({each[:8] : log_path + folder + '/' + each for each in os.listdir(log_path + folder + '/')}) \n",
    "\n",
    "tmp = os.listdir(ds_path)\n",
    "ds_dict = {}\n",
    "for folder in tmp:\n",
    "    ds_dict.update({each[:8] : ds_path + folder + '/' + each for each in os.listdir(ds_path + folder + '/')})\n",
    "    \n",
    "df_dict = os.listdir(df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55733afb",
   "metadata": {},
   "source": [
    "## For each sample's feature computation, the steps are:\n",
    "#### 1: Read the sampel as a dataframe\n",
    "#### 2: Calculate each feature and save it as a new column\n",
    "#### 3: Save the dataframe as a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446a5b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def read_graph(name):\n",
    "    with open(name) as f:\n",
    "        lines = f.readlines()\n",
    "        arcs = []\n",
    "        for line in lines:\n",
    "            if line == '\\n': \n",
    "                continue\n",
    "            parts = line.split()\n",
    "            det = parts[0]\n",
    "            if det == 'Name':\n",
    "                name = parts[1]\n",
    "            elif det == 'Nodes':\n",
    "                n_vertices = int(parts[1])\n",
    "            elif det == 'Edges':\n",
    "                n_edges = int(parts[1])\n",
    "            elif det == 'E':\n",
    "                i = int(parts[1])\n",
    "                j = int(parts[2])\n",
    "                c = int(parts[3])\n",
    "                arcij = ((i,j),c)\n",
    "                arcs.append(arcij)\n",
    "            elif det == 'Terminals':\n",
    "                n_terminals = int(parts[1])\n",
    "        vertices = np.arange(1, int(n_vertices)+1)\n",
    "        vertices = vertices.tolist()\n",
    "        terminals = np.arange(1, int(n_terminals)+1)\n",
    "        terminals = terminals.tolist()\n",
    "        assert(int(n_edges) == len(arcs))\n",
    "    f.close()\n",
    "    ### The format of graphs is D=(V,A,R)\n",
    "    return [vertices, arcs, terminals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0268e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def read_log(target):\n",
    "    with open(target) as f:\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "        ilp_rt = lines[0]\n",
    "        ilp_c = lines[1]\n",
    "        lp_rt = lines[2]\n",
    "        lp_c = lines[3]\n",
    "        # lines[4] is whether ilp_c == lp_c\n",
    "        sols = lines[5:]\n",
    "        b = sols.index('')\n",
    "        ilp_sol = [re.sub(\"[()',]\",\" \", term).split() for term in sols[:b]]\n",
    "        lp_sol = [re.sub(\"[()',]\",\" \", term).split() for term in sols[b+1:]]\n",
    "    return {\"ilp_rt\" : ilp_rt, \"ilp_c\" : ilp_c, \"ilp_sol\" : ilp_sol, \"lp_rt\" : lp_rt, \"lp_c\" : lp_c, \"lp_sol\" : lp_sol}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9954e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import timeit\n",
    "def generate_df(ds_target, log_target):\n",
    "    vertices, arcs, terminals = read_graph(ds_target)\n",
    "    df = pd.DataFrame(columns = ['Node1', 'Node2', 'Weight'])\n",
    "    for arc in arcs:\n",
    "        node = arc[0]\n",
    "        df = df.append({'Node1' : node[0] , 'Node2' : node[1], 'Weight' : arc[1]}, ignore_index=True)\n",
    "        \n",
    "    # Label:\n",
    "    log = read_log(log_target)\n",
    "    ilp_sol = log['ilp_sol']\n",
    "    df.insert(3,'Label',np.zeros(len(df),dtype=np.float32))\n",
    "    for i,j,v in ilp_sol:\n",
    "        i,j = int(i), int(j)\n",
    "        df.loc[((df['Node1'] == i) & (df['Node2'] == j)) | ((df['Node1'] == j) & (df['Node2'] == i)), 'Label'] = v\n",
    "    \n",
    "    # Feature 0: LP Relaxation\n",
    "    lp_sol = log['lp_sol']\n",
    "    df.insert(3,'LP',np.zeros(len(df),dtype=np.float32))\n",
    "    for i,j,v in lp_sol:\n",
    "        i,j = int(i), int(j)\n",
    "        df.loc[((df['Node1'] == i) & (df['Node2'] == j)) | ((df['Node1'] == j) & (df['Node2'] == i)), 'LP'] = v\n",
    "      \n",
    "    start = timeit.default_timer()\n",
    "    # Feature 1: Normalized weight by dividing the max weight\n",
    "    tmp = df['Weight'].max()\n",
    "    col = df['Weight'] / tmp\n",
    "    df.insert(3,'Normalized Weight',col)\n",
    "    \n",
    "    # Feature 2: Variance of Normalizaed weight\n",
    "    avg = df['Normalized Weight'].mean()\n",
    "    col = (df['Normalized Weight'] - avg)**2\n",
    "    df.insert(3,'Variance',col)\n",
    "    \n",
    "    # Feature 3: Local rank for edge (i,j) on vertex i and vertex j\n",
    "    # Loop every vertex\n",
    "    df.insert(3,'Local Rank i',np.zeros(len(df),dtype=np.int8))\n",
    "    df.insert(3,'Local Rank j',np.zeros(len(df),dtype=np.int8))\n",
    "    for i in range(1, max(df['Node1'].max(),df['Node2'].max()) + 1):\n",
    "        loc_edges = df.loc[(df['Node1'] == i) | (df['Node2'] == i)]\n",
    "        loc_edges = loc_edges.sort_values(by=['Weight'])\n",
    "        loc_edges = loc_edges.reset_index(drop=True)\n",
    "        for index, row in loc_edges.iterrows():\n",
    "            # When the current vertex is the i vertex for this edge\n",
    "            if( row['Node1'] == i):\n",
    "                df.loc[(df['Node1'] == row['Node1']) & (df['Node2'] == row['Node2']), ['Local Rank i']] = (index + 1)/(loc_edges.index.max() + 1)\n",
    "            # When the current vertex is the j vertex for this edge\n",
    "            else:\n",
    "                df.loc[(df['Node1'] == row['Node1']) & (df['Node2'] == row['Node2']), ['Local Rank j']] = (index + 1)/(loc_edges.index.max() + 1)\n",
    "    \n",
    "    # Create Graph object\n",
    "    G = nx.Graph()\n",
    "    for index, row in df.iterrows():\n",
    "        i = row['Node1']\n",
    "        j = row['Node2']\n",
    "        G.add_edge(i,j)\n",
    "        \n",
    "    # Feature 4: Degree Centrality\n",
    "    cen = nx.degree_centrality(G)\n",
    "    df.insert(3,'Degree Centrality i',np.zeros(len(df)))\n",
    "    df.insert(3,'Degree Centrality j',np.zeros(len(df)))\n",
    "    for key in cen:\n",
    "        df.loc[df['Node1'] == key, 'Degree Centrality i'] = cen[key]\n",
    "        df.loc[df['Node2'] == key, 'Degree Centrality j'] = cen[key]\n",
    "    \n",
    "    df.insert(3,'Degree Centrality Max',np.zeros(len(df)))\n",
    "    df.insert(3,'Degree Centrality Min',np.zeros(len(df)))\n",
    "    for index, row in df.iterrows():\n",
    "        df.loc[index, 'Degree Centrality Max'] = max(row['Degree Centrality i'], row['Degree Centrality j'])\n",
    "        df.loc[index, 'Degree Centrality Min'] = min(row['Degree Centrality i'], row['Degree Centrality j'])\n",
    "    df = df.drop(columns=['Degree Centrality i','Degree Centrality j'])\n",
    "    \n",
    "    # Feature 5:Betweenness Centrality\n",
    "    cen = nx.betweenness_centrality(G,k=10,normalized=True,weight='Weight')\n",
    "    df.insert(len(df.columns)-1,'between_cen_i',np.zeros(len(df)))\n",
    "    df.insert(len(df.columns)-1,'between_cen_j',np.zeros(len(df)))\n",
    "    for key in cen:\n",
    "        df.loc[df['Node1'] == key, 'between_cen_i'] = cen[key]\n",
    "        df.loc[df['Node2'] == key, 'between_cen_j'] = cen[key]\n",
    "    \n",
    "    df.insert(3,'Betweenness Centrality Max',np.zeros(len(df)))\n",
    "    df.insert(3,'Betweenness Centrality Min',np.zeros(len(df)))\n",
    "    for index, row in df.iterrows():\n",
    "        df.loc[index, 'Betweenness Centrality Max'] = max(row['between_cen_i'], row['between_cen_j'])\n",
    "        df.loc[index, 'Betweenness Centrality Min'] = min(row['between_cen_i'], row['between_cen_j'])\n",
    "    df = df.drop(columns=['between_cen_i','between_cen_j'])\n",
    "    \n",
    "    # Feature 6: Eigenvector Centrality\n",
    "    cen = nx.eigenvector_centrality(G,max_iter=99999,weight='Weight')\n",
    "    df.insert(len(df.columns)-1,'eigen_cen_i',np.zeros(len(df)))\n",
    "    df.insert(len(df.columns)-1,'eigen_cen_j',np.zeros(len(df)))\n",
    "    for key in cen:\n",
    "        df.loc[df['Node1'] == key, 'eigen_cen_i'] = cen[key]\n",
    "        df.loc[df['Node2'] == key, 'eigen_cen_j'] = cen[key]\n",
    "        \n",
    "    df.insert(3,'Eigenvector Centrality Max',np.zeros(len(df)))\n",
    "    df.insert(3,'Eigenvector Centrality Min',np.zeros(len(df)))\n",
    "    for index, row in df.iterrows():\n",
    "        df.loc[index, 'Eigenvector Centrality Max'] = max(row['eigen_cen_i'], row['eigen_cen_j'])\n",
    "        df.loc[index, 'Eigenvector Centrality Min'] = min(row['eigen_cen_i'], row['eigen_cen_j'])\n",
    "    df = df.drop(columns=['eigen_cen_i','eigen_cen_j'])\n",
    "    stop = timeit.default_timer()\n",
    "    fe_rt = stop - start + float(log[\"lp_rt\"])\n",
    "    \n",
    "    # Create the runtime df\n",
    "    se = pd.Series({'ILP Objective':float(log['ilp_c']), 'ILP Runtime':float(log['ilp_rt']), 'FE Runtime':fe_rt})\n",
    "    \n",
    "    return df, se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd83d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature():\n",
    "    target_list = []\n",
    "    for each in list(log_dict.keys()):\n",
    "        log_target = log_dict[each]\n",
    "        with open(log_target) as f:\n",
    "            if f.readlines()[4].startswith(\"F\"):\n",
    "                target_list.append(each)\n",
    "    df = pd.DataFrame()\n",
    "    for each in target_list:\n",
    "        log_target = log_dict[each]\n",
    "        ds_target = ds_dict[each]\n",
    "        fe_df, se = generate_df(ds_target, log_target)\n",
    "        path = df_path+each+df_suffix\n",
    "        fe_df.to_csv(path, index=False)\n",
    "        df = df.append(se.rename(each))\n",
    "    path = df_path+\"evaluation.csv\"\n",
    "    df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef96f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_feature()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7a676abbc58fb72a4139e964996bdbf10c701e0b1ca7f6ed9a8294b6eab108e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
